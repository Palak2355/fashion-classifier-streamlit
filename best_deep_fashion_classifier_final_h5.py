# -*- coding: utf-8 -*-
"""best_deep_fashion_classifier_final.h5

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11K_Gbc1wzID5zq8ktLftaWVm9wkV6qhN
"""

import tensorflow as tf
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping # NEW: For runtime optimization
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score
from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
import xgboost as xgb
import keras_tuner as kt
import ipywidgets as widgets
from IPython.display import display, clear_output
from PIL import Image
import io

# Define the 10 class names for Fashion-MNIST
class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Shirt', 'Dress',
               'Coat', 'Sandal', 'Sneaker', 'Bag', 'Ankle boot']

print("All libraries imported successfully!")
print("TensorFlow version:", tf.__version__)
print("--- Check Runtime: GPU should be enabled for fast training! ---")

==============================================================================

# --- Load Data from CSV ---
try:
    train_df = pd.read_csv('fashion-mnist_train.csv')  # Adjust path if needed
    test_df = pd.read_csv('fashion-mnist_test.csv')
    print(f"\nTraining data shape: {train_df.shape}")
    print(f"Test data shape: {test_df.shape}")
except FileNotFoundError:
    print("\nðŸš¨ ERROR: CSV files not found. Please verify the path or upload files to your session.")
    raise

# Separate Features (X) and Labels (y)
X_train_full_csv = train_df.iloc[:, 1:].values.astype('float32')
y_train_full = train_df.iloc[:, 0].values
X_test_csv = test_df.iloc[:, 1:].values.astype('float32')
y_test = test_df.iloc[:, 0].values

# --- Preprocessing ---
X_train_full = X_train_full_csv / 255.0
X_test = X_test_csv / 255.0

# Split Training data into Train and Validation sets
X_train, X_valid, y_train, y_valid = train_test_split(
    X_train_full, y_train_full, test_size=0.1, random_state=42
)

# Reshape for EDA visualization
X_train_img = X_train.reshape(-1, 28, 28)

plt.figure(figsize=(10, 5))
for i in range(10):
    plt.subplot(2, 5, i + 1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(X_train_img[i], cmap=plt.cm.binary)
    plt.xlabel(class_names[y_train[i]])
plt.suptitle("Sample Fashion-MNIST Images (Loaded from CSV)")
plt.show()


print("\n--- Training Multiple Models ---")

models = {
    'Logistic Regression': LogisticRegression(solver='saga', max_iter=200, random_state=42, n_jobs=-1, verbose=0),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Random Forest': RandomForestClassifier(random_state=42, n_jobs=-1),
    'SVM': SVC(random_state=42),
    'MLP': MLPClassifier(random_state=42, max_iter=200),
    'XGBoost': xgb.XGBClassifier(random_state=42, n_jobs=-1),
    'AdaBoost': AdaBoostClassifier(random_state=42)
}

accuracies_before = {}
accuracies_after = {}

for name, model in models.items():
    print(f"\n--- Training {name} ---")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    accuracies_before[name] = acc
    print(f"âœ… {name} Test Accuracy (Before Tuning): {acc:.4f}")

# Hyperparameter Tuning
print("\n--- Hyperparameter Tuning ---")

# Logistic Regression
lr_param_grid = {'C': [0.1, 1, 10], 'solver': ['liblinear', 'saga']}
lr_grid = GridSearchCV(LogisticRegression(max_iter=200, random_state=42, n_jobs=-1), lr_param_grid, cv=3, scoring='accuracy')
lr_grid.fit(X_train, y_train)
lr_tuned = lr_grid.best_estimator_
lr_tuned_pred = lr_tuned.predict(X_test)
accuracies_after['Logistic Regression'] = accuracy_score(y_test, lr_tuned_pred)
print(f"âœ… Logistic Regression Tuned Accuracy: {accuracies_after['Logistic Regression']:.4f}")

# Decision Tree
dt_param_grid = {'max_depth': [None, 10, 20], 'min_samples_split': [2, 5, 10]}
dt_grid = GridSearchCV(DecisionTreeClassifier(random_state=42), dt_param_grid, cv=3, scoring='accuracy')
dt_grid.fit(X_train, y_train)
dt_tuned = dt_grid.best_estimator_
dt_tuned_pred = dt_tuned.predict(X_test)
accuracies_after['Decision Tree'] = accuracy_score(y_test, dt_tuned_pred)
print(f"âœ… Decision Tree Tuned Accuracy: {accuracies_after['Decision Tree']:.4f}")

# Random Forest
rf_param_grid = {'n_estimators': [50, 100], 'max_depth': [None, 10]}
rf_grid = GridSearchCV(RandomForestClassifier(random_state=42, n_jobs=-1), rf_param_grid, cv=3, scoring='accuracy')
rf_grid.fit(X_train, y_train)
rf_tuned = rf_grid.best_estimator_
rf_tuned_pred = rf_tuned.predict(X_test)
accuracies_after['Random Forest'] = accuracy_score(y_test, rf_tuned_pred)
print(f"âœ… Random Forest Tuned Accuracy: {accuracies_after['Random Forest']:.4f}")

# SVM
svm_param_grid = {'C': [0.1, 1], 'kernel': ['linear', 'rbf']}
svm_grid = GridSearchCV(SVC(random_state=42), svm_param_grid, cv=3, scoring='accuracy')
svm_grid.fit(X_train, y_train)
svm_tuned = svm_grid.best_estimator_
svm_tuned_pred = svm_tuned.predict(X_test)
accuracies_after['SVM'] = accuracy_score(y_test, svm_tuned_pred)
print(f"âœ… SVM Tuned Accuracy: {accuracies_after['SVM']:.4f}")

# MLP
mlp_param_grid = {'hidden_layer_sizes': [(100,), (50, 50)], 'alpha': [0.0001, 0.001]}
mlp_grid = GridSearchCV(MLPClassifier(random_state=42, max_iter=200), mlp_param_grid, cv=3, scoring='accuracy')
mlp_grid.fit(X_train, y_train)
mlp_tuned = mlp_grid.best_estimator_
mlp_tuned_pred = mlp_tuned.predict(X_test)
accuracies_after['MLP'] = accuracy_score(y_test, mlp_tuned_pred)
print(f"âœ… MLP Tuned Accuracy: {accuracies_after['MLP']:.4f}")

# XGBoost
xgb_param_grid = {'n_estimators': [50, 100], 'max_depth': [3, 6]}
xgb_grid = GridSearchCV(xgb.XGBClassifier(random_state=42, n_jobs=-1), xgb_param_grid, cv=3, scoring='accuracy')
xgb_grid.fit(X_train, y_train)
xgb_tuned = xgb_grid.best_estimator_
xgb_tuned_pred = xgb_tuned.predict(X_test)
accuracies_after['XGBoost'] = accuracy_score(y_test, xgb_tuned_pred)
print(f"âœ… XGBoost Tuned Accuracy: {accuracies_after['XGBoost']:.4f}")

# AdaBoost
ada_param_grid = {'n_estimators': [50, 100], 'learning_rate': [0.1, 1]}
ada_grid = GridSearchCV(AdaBoostClassifier(random_state=42), ada_param_grid, cv=3, scoring='accuracy')
ada_grid.fit(X_train, y_train)
ada_tuned = ada_grid.best_estimator_
ada_tuned_pred = ada_tuned.predict(X_test)
accuracies_after['AdaBoost'] = accuracy_score(y_test, ada_tuned_pred)
print(f"âœ… AdaBoost Tuned Accuracy: {accuracies_after['AdaBoost']:.4f}")



# --- Prepare data for CNN ---
# Reshape data from (N, 784) to (N, 28, 28, 1) for CNN input
X_train_cnn = X_train.reshape(-1, 28, 28, 1)
X_valid_cnn = X_valid.reshape(-1, 28, 28, 1)
X_test_cnn = X_test.reshape(-1, 28, 28, 1)

# Convert labels to one-hot encoding
y_train_cat = to_categorical(y_train, num_classes=10)
y_valid_cat = to_categorical(y_valid, num_classes=10)


datagen = ImageDataGenerator(
    rotation_range=8,
    zoom_range=0.08,
    width_shift_range=0.08, # Corrected argument
    height_shift_range=0.08 # Corrected argument
)
datagen.fit(X_train_cnn)
print("\nData Augmentation Generator configured.")

# Monitors validation loss and stops training if no improvement after 5 epochs
early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True
)


print("\n--- Training DEEPER Convolutional Neural Network (CNN) ---")

deep_cnn_model = Sequential([
    # Block 1
    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    MaxPooling2D((2, 2)),

    # Block 2
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),

    # NEW Block 3 (Deeper Architecture - Innovation)
    Conv2D(128, (3, 3), activation='relu'),
    # Note: No Pooling here as the image is already small.

    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5), # Regularization
    Dense(10, activation='softmax')
])


deep_cnn_model.compile(optimizer='adam',
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

deep_cnn_model.summary()

# Train the model with augmented data, Early Stopping, and MAX 20 Epochs
print("\nStarting DEEPER CNN Training (Max 20 Epochs, will stop early if performance plateaus)...")
history = deep_cnn_model.fit(
    datagen.flow(X_train_cnn, y_train_cat, batch_size=128),
    steps_per_epoch=len(X_train_cnn) // 128,
    epochs=20, # <--- MAX EPOCHS SET TO 20
    validation_data=(X_valid_cnn, y_valid_cat),
    callbacks=[early_stopping], # Early Stopping controls effective runtime
    verbose=1
)

# Evaluation
cnn_loss, cnn_accuracy = deep_cnn_model.evaluate(X_test_cnn, to_categorical(y_test), verbose=0)
print(f"\nâœ… Final DEEPER CNN Test Accuracy (Before Tuning): {cnn_accuracy:.4f}")
accuracies_before['CNN'] = cnn_accuracy

# Hyperparameter Tuning for Deep CNN
print("\n--- Hyperparameter Tuning for Deep CNN ---")

def build_model(hp):
    model = Sequential()
    # Tune number of filters in first layer
    model.add(Conv2D(hp.Int('conv1_filters', min_value=32, max_value=128, step=32), (3, 3), activation='relu', input_shape=(28, 28, 1)))
    model.add(MaxPooling2D((2, 2)))
    # Tune number of filters in second layer
    model.add(Conv2D(hp.Int('conv2_filters', min_value=64, max_value=256, step=64), (3, 3), activation='relu'))
    model.add(MaxPooling2D((2, 2)))
    # Tune number of filters in third layer
    model.add(Conv2D(hp.Int('conv3_filters', min_value=128, max_value=512, step=128), (3, 3), activation='relu'))
    model.add(Flatten())
    # Tune dense units
    model.add(Dense(hp.Int('dense_units', min_value=64, max_value=256, step=64), activation='relu'))
    # Tune dropout rate
    model.add(Dropout(hp.Float('dropout', min_value=0.2, max_value=0.5, step=0.1)))
    model.add(Dense(10, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

tuner = kt.Hyperband(build_model, objective='val_accuracy', max_epochs=10, factor=3, directory='tuner_dir', project_name='fashion_cnn')
tuner.search(datagen.flow(X_train_cnn, y_train_cat, batch_size=128), epochs=10, validation_data=(X_valid_cnn, y_valid_cat), callbacks=[early_stopping])

# Get the best model
best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]
print(f"Best hyperparameters: {best_hps.values}")

cnn_tuned_model = tuner.hypermodel.build(best_hps)
cnn_tuned_model.fit(datagen.flow(X_train_cnn, y_train_cat, batch_size=128), epochs=20, validation_data=(X_valid_cnn, y_valid_cat), callbacks=[early_stopping])

# Evaluation after tuning
cnn_tuned_loss, cnn_tuned_accuracy = cnn_tuned_model.evaluate(X_test_cnn, to_categorical(y_test), verbose=0)
print(f"\nâœ… Final DEEPER CNN Test Accuracy (After Tuning): {cnn_tuned_accuracy:.4f}")
accuracies_after['CNN'] = cnn_tuned_accuracy

# Save the best tuned model
model_path = 'best_deep_fashion_classifier_final.h5'
cnn_tuned_model.save(model_path)
print(f"Best tuned model saved to {model_path}")

# Use the tuned model accuracy
deep_cnn_accuracy = cnn_tuned_accuracy

# Create a DataFrame for comparison
models_list = []
accuracies_list = []
notes_list = []

for model_name in accuracies_before.keys():
    models_list.append(f"{model_name} (Before Tuning)")
    accuracies_list.append(accuracies_before[model_name])
    notes_list.append(f"Baseline {model_name}")

for model_name in accuracies_after.keys():
    models_list.append(f"{model_name} (After Tuning)")
    accuracies_list.append(accuracies_after[model_name])
    notes_list.append(f"Tuned {model_name}")

results = pd.DataFrame({
    'Model': models_list,
    'Test Accuracy': accuracies_list,
    'Notes': notes_list
})

print("\n--- Model Comparative Analysis ---")
print(results.sort_values(by='Test Accuracy', ascending=False).to_markdown(index=False))

# Plot accuracies
import matplotlib.pyplot as plt

models_before = list(accuracies_before.keys())
acc_before = list(accuracies_before.values())
acc_after = [accuracies_after[model] for model in models_before]

x = range(len(models_before))
plt.figure(figsize=(12, 6))
plt.bar(x, acc_before, width=0.4, label='Before Tuning', align='center')
plt.bar([i + 0.4 for i in x], acc_after, width=0.4, label='After Tuning', align='center')
plt.xlabel('Models')
plt.ylabel('Test Accuracy')
plt.title('Model Accuracies Before and After Hyperparameter Tuning')
plt.xticks([i + 0.2 for i in x], models_before, rotation=45)
plt.legend()
plt.tight_layout()
plt.show()

# Load the saved best model for deployment
model_path = 'best_deep_fashion_classifier_final.h5'
try:
    best_model = load_model(model_path)
    print(f"\nLoaded new best model: {model_path}")
except Exception as e:
    print(f"\nFailed to load model for GUI. Ensure training completed successfully. Error: {e}")
    best_model = deep_cnn_model

# 5.1 Define the prediction function (No change needed here)
def predict_image(image_data):
    # Open, convert to grayscale, and resize to 28x28
    img = Image.open(io.BytesIO(image_data)).convert('L')
    img = img.resize((28, 28))

    # Convert to array, normalize, and reshape (1, 28, 28, 1)
    img_array = np.array(img, dtype=np.float32) / 255.0
    img_array = img_array.reshape(1, 28, 28, 1)

    # Predict
    predictions = best_model.predict(img_array, verbose=0)
    predicted_class_index = np.argmax(predictions[0])
    confidence = predictions[0][predicted_class_index] * 100

    return class_names[predicted_class_index], confidence


file_upload = widgets.FileUpload(
    accept='image/*',
    multiple=False,
    description='Upload a Fashion Item Image'
)

output_area = widgets.Output()
button = widgets.Button(description="Classify Item")


def on_classify_button_clicked(b):
    with output_area:
        clear_output(wait=True)
        if not file_upload.value:
            print(" Please upload an image file first.")
            return

        # Extract file data
        uploaded_file_name = next(iter(file_upload.value))
        image_data = file_upload.value[uploaded_file_name]['content']

        try:
            # Display uploaded image
            img = Image.open(io.BytesIO(image_data))
            print(f"Uploaded Image: **{uploaded_file_name}**")
            display(img.resize((150, 150)))

            # Predict
            predicted_label, confidence = predict_image(image_data)

            # Display results
            print("\n--- Prediction Result ---")
            print(f"Predicted Class: **{predicted_label}**")
            print(f"Confidence: **{confidence:.2f}%**")

            # Contextual feedback
            if confidence < 75.0:
                 print("\n Note: Confidence is low. The model may be struggling with the complex color/background of this image versus the simple training images.")

        except Exception as e:
            print(f" An error occurred during classification. Error: {e}")

# 5.4 Link the button to the function
button.on_click(on_classify_button_clicked)

# 5.5 Display the GUI layout
gui_title = widgets.HTML("<h2> Fashion Classifier GUI (Deployment)</h2>")
gui_vbox = widgets.VBox([gui_title, file_upload, button, output_area])

print("\n--- Interact with the GUI below to test the final model ---")
display(gui_vbox)



